
It's like a measure of **how wrong your guesses are.**
The more wrong ⏫ ⇾ ⏫ Cross entropy.

$$ L = - \frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)$$
> [!info] Info #card
> Can also be though of as -1*[[Log-Likelihood]]

